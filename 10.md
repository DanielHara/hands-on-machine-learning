# Answers to Chapter 10: Introduction to Artificial Neural Networks with Keras

## Question 3
Because a classic perceptron uses the step function in the output (`step(WX + b)`), and not a sigmoid function (`f(x) = 1 / (1 + exp(-x))`), like in logistic regression.
The step function is a hard function: it just outputs 0 and 1, which means just a hard prediction, without a sense of how certain the model is of its prediction.
On the other hand, the sigmoid function varies smoothly from 0 to 1, whereby 0.51 would be ranked as a prediction of the positive class with little certainty, while 0.9999 would hint
at the model being very convinced of its prediction.
Therefore, linear regression makes a more nuanced prediction and adds more information to it (i.e., a measure of how convinced the model is of its prediction), when compared to a classic perceptron.

## Question 10
> See if you can get over 98% accuracy by manually tuning the hyperparamters.

I tried this:
```python
import tensorflow as tf
import keras_tuner

fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()

(X_train_fill, y_train_full), (X_test, Y_test) = fashion_mnist
X_train, y_train = X_train_fill[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_fill[-5000:], y_train_full[-5000:]

tf.random.set_seed(42)

model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=[28,28]))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(300, activation='relu'))
model.add(tf.keras.layers.Dense(100, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))
opt = tf.keras.optimizers.SGD(0.02, clipnorm=1.)
model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid))
```
, and I get:
```
Epoch 70/100
1719/1719 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - accuracy: 0.9806 - loss: 0.0634 - val_accuracy: 0.8758 - val_loss: 1.0834
```



```
